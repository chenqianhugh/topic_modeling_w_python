{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/Profile_pic_new.png\" width=\"200\">\n",
        "\n",
        "<h1><center> <font color='green'>Topic Modeling in Python</font></center></h1> \n",
        "\n",
        "<div style=\"font-size: 20px\">\n",
        "Speaker: Qian Chen</div>\n",
        "\n",
        "<div style=\"font-size: 16px\">\n",
        "Date: 2020-02-26\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> <font color='green'>Introduction</font></center></h1>\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <h2><center> <font color='green'>About Me</font></center></h2>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-size: 20px\">\n",
        "<ul>\n",
        "<li>Data Scientist at Anthem</li>\n",
        "<li>Work at Willis Tower, Chicago</li>\n",
        "<li>Support Program Integrity Work</li>\n",
        "</ul>\n",
        "</div>    "
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/anthem_logo.jpg\" width=\"400\">\n",
        "\n",
        "<h2><center> <font color='green'>About Anthem</font></center></h2>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-size: 20px\">\n",
        "<ul>\n",
        "<li>2nd largest health insurer in US</li>\n",
        "<li>For-profit and public company (NYSE: ANTM) in the Blue Cross Blue Shield Association</li>\n",
        "<li>As of 2018, Anthem has approximately 40 million members</li>\n",
        "</ul>\n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <h2><center> <font color='green'>About this talk</font></center></h2>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-size: 20px\">\n",
        "<ul>\n",
        "<li>Topic Modeling Introduction</li>\n",
        "<li>Latent Dirichlet Allocation (LDA) Algorithm</li>\n",
        "<li>Implementation with Python</li>\n",
        "<li>Other Algorithms</li>\n",
        "<li>Discussion</li>    \n",
        "</ul>\n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <img style=\"float: top;\" src=\"books_new3.jpeg\" width=\"800\">"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "slideshow": {
          "slide_type": "skip"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <h1><center> <font color='green'>Topic Modeling</font></center></h1>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <h2><center> <font color='green'>What is topic modeling?</font></center></h2>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"font-size: 20px\">\n",
        "<li>\"A topic model is a type of statistical model for <span class=\"fragment highlight-red\">discovering</span> the abstract <span class=\"fragment highlight-red\">\"topics\"</span> that occur in a collection of <span class=\"fragment highlight-red\">documents</span>\" <a href=\"http://en.wikipedia.org/wiki/Topic_model\">[1]: Wikipedia Page</a></li>\n",
        "<li>\"Topic models are a suite of algorithms that uncover the <span class=\"fragment highlight-red\">hidden thematic structure</span> in document collections. These algorithms help us develop new ways to <span class=\"fragment highlight-red\">search, browse</span> and summarize large archives of texts\" <a href=\"https://user.eng.umd.edu/~smiran/LDA.pdf\">[2]:  Academic Slides</a></li>\n",
        "<!-- <li>Topic models provide a simple way to analyze large volumes of <span class=\"fragment highlight-red\">unlabeled</span> text. A \"topic\" consists of a <span class=\"fragment highlight-red\">cluster</span> of words that <span class=\"fragment highlight-red\">frequently</span> occur together<a href=\"http://mallet.cs.umass.edu/topics.php\">[3]</a></li> -->\n",
        "<li>\"Topic modeling is an <span class=\"fragment highlight-red\">unsupervised</span> technique that intends to analyze large volumes of <span class=\"fragment highlight-red\">unlabeled</span> text by <span class=\"fragment highlight-red\">clustering</span> the documents into groups\" <a href=\"https://stackabuse.com/python-for-nlp-topic-modeling/\">[3]: Data Science Blog</a></li>\n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "<h3><left> <font color='green'>Keywords:</font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>Exploratory: unlabled, unsupervised, discover, search, browse</li>\n",
        "<li>Latent Variables: hidden thematic structure, topics</li>\n",
        "<li>Clustering: cluster of words, document collection</li>\n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <div style=\"font-size: 18px\">\n",
        "# <p>\n",
        "# <a href=\"http://en.wikipedia.org/wiki/Topic_model\">[1]: Wikipedia Page</a><br>\n",
        "# <a href=\"https://user.eng.umd.edu/~smiran/LDA.pdf\">[2]:  Acamedic Slides</a><br>\n",
        "# <!-- <a href=\"http://mallet.cs.umass.edu/topics.php\">http://mallet.cs.umass.edu/topics.php</a></p> -->\n",
        "# <a href=\"https://stackabuse.com/python-for-nlp-topic-modeling/\">[3]: Data Science Blog</a><br>    \n",
        "# </div"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "slideshow": {
          "slide_type": "skip"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img style=\"float: top;\" src=\"images/clustering.png\" width=\"800\" length=\"1000\">"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> <font color='green'>Latent Dirichlet Allocation (LDA)</font></center></h1>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Definitions of LDA:</font></left></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>A form of <span class=\"fragment highlight-red\">unsupervised learning</span> that views documents as <span class=\"fragment highlight-red\">bags of words</span> (ie the order of the words, the grammatical role of the words (e.g. conjunctionÔºöand/but/or..., articles: a/an/the) does not matter)</li>\n",
        "<li> A document is composed of <span class=\"fragment highlight-red\">a set of topics</span> and then for each topic picking <span class=\"fragment highlight-red\">a set of \"important\" or \"effective\" words</span></li>\n",
        "</div>\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>LDA plate diagram</font></left></h3>\n",
        "\n", 
         "<img src=\"images/lda_plate_notation.png\">\n",
        "<center><a lefthref=\"https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">Smoothed LDA from https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation </a></center>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div style=\"font-size: 16px\">\n",
        "Above is what is known as a plate diagram of an LDA model where:\n",
        "<li>Œ± is the per-document topic distributions</li>\n",
        "<li>Œ≤ is the per-topic word distribution</li>\n",
        "<li>Œ∏ is the topic distribution for document m</li>\n",
        "<li>œÜ is the word distribution for topic k</li>\n",
        "<li>z is the topic for the n-th word in document m </li>\n",
        "<li>w is the specific word</li>\n",
        "</div>\n",
        "    \n",
        "    \n",
        "<!-- <li>From a dirichlet distribution <b>Dir(Œ±)</b>, we draw a random sample representing the topic distribution, or topic mixture, of a particular document. This topic distribution is <b>Œ∏</b>. From <b>Œ∏</b>, we select a particular topic <b>Z</b> based on the distribution</li>\n",
        "<li> Next, from another dirichlet distribution <b>Dir(ùõΩ)</b>, we select a random sample representing the word distribution of the topic <b>Z</b>. This word distribution is <b>œÜ</b>. From <b>œÜ</b>, we choose the word <b>w</b></li> -->\n",
        "\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>The iterative algorithm:</font></left></h3>\n",
        "\n",
        "![lda_plate_notation.png](attachment:lda_plate_notation.png)\n",
        "<hr>\n",
        "\n",
        "<div style=\"font-size: 16px\">\n",
        "<ol>\n",
        "<li>Initialize parameters </li>\n",
        "<li>Initialize topic assignments randomly from the distribution of <b>Œ±</b></li>\n",
        "<li>Iterate</li>\n",
        "<ul>For each word in each document:\n",
        "    <li>Resample topic for word, given all other words and their current topic assignments</li>\n",
        "    <li>Probabilistically assign word <b>w</b> a topic based on two things:\n",
        "-- which topics are in document <b>m</b>\n",
        "-- how many times word <b>w</b> has been assigned a particular topic across all of the documents (this distribution is called <b>Œ≤</b>)</li></ul>\n",
        "<li>Repeat this process a number of times for each document and get results</li>\n",
        "<li>Evaluate model</li>\n",
        "</ol>\n",
        "</div>\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Iterative algorithm:</font></center></h3>\n",
        "\n",
        "![lda_plate_notation.png](attachment:lda_plate_notation.png)\n",
        "\n",
        "    <li>Assume there are <b>k</b> topics across all of the documents</li>\n",
        "<li>Distribute these <b>k</b> topics across document <b>m</b> (this distribution is known as <b>Œ±</b> and can be symmetric or asymmetric) by assigning each word a topic</li>\n",
        "<li>For each word <b>w</b> in document <b>m</b>, assume its topic is wrong but every other word is assigned the correct topic</li>\n",
        "<li>Probabilistically assign word <b>w</b> a topic based on two things:\n",
        "-- which topics are in document <b>m</b>\n",
        "-- how many times word <b>w</b> has been assigned a particular topic across all of the documents (this distribution is called <b>Œ≤</b>)</li>\n",
        "<li>Repeat this process a number of times for each document and you‚Äôre done</li>\n",
        "     \n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Evaluate model: Human-in-the-loop</font></left></h3>\n",
        "\n",
        "<div style=\"font-size:18px\">\n",
        "<li><b>Word intrusion [1]:</b> For each trained topic, take first ten words, substitute one of them with another, randomly chosen word (intruder!) and see whether a human can reliably tell which one it was. If so, the trained topic is topically coherent (good); if not, the topic has no discernible theme (bad) [2]</li>\n",
        "<br>\n",
        "<li><b>Topic intrusion:</b> Subjects are shown the title and a snippet from a document. Along with the document they are presented with four topics. Three of those topics are the highest probability topics assigned to that document. The remaining intruder topic is chosen randomly from the other\n",
        "low-probability topics in the model [1]</li></div>\n",
        "\n",
        "<hr>\n",
        "<div style=\"font-size: 16px\">\n",
        "<p>[1] - <a href=\"http://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf\">How Humans Interpret Topic Models</a><br>\n",
        "[2] - <a href=\"http://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html\">Topic Modeling for Fun and Profit</a><br>\n",
        "</p>\n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Evaluate model: Metrics</font></left></h3>\n",
        "\n",
        "<div style=\"font-size:20px\">\n",
        "<li>Co-doc Coherence</li>\n",
        "<li>Size (#\tof tokens assigned)</li>\n",
        "<li>Within-doc rank</li>\n",
        "<li>Similarity to corpus-wide distribution</li>\n",
        "<li>Locally-frequent words</li>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div style=\"font-size: 18px\">\n",
        "    <p><b>Good reference</b> <a href=\"http://mimno.infosci.cornell.edu/slides/details.pdf\">http://mimno.infosci.cornell.edu/slides/details.pdf</a><br></p></div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> <font color='green'>Implementation of LDA with Python\n",
        "</font></center></h1>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Modeling steps</font></center></h3>\n",
        "<img src=\"images/pipeline_modeling.png\">"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Example data</font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>The dataset contains <b>Amazon</b> user reviews for different products in the <b>food</b> category</li>\n",
        "<li>The data can be downloaded from <a href=\"https://www.kaggle.com/sdxingaijing/topic-model-lda-algorithm/data\">Kaggle</a>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "```python\n",
        "df_reviews = pd.read_csv('/home/ag27675/Disc2_QC/Data/Amazon_Reviews.csv')\n",
        "df_reviews.head(5)\n",
        "```\n",
        "\n",
        "![amazon_reviews_screenshot.png](attachment:amazon_reviews_screenshot.png)"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Python libraries</font></center></h3>\n",
        "<hr>\n",
        "<!-- <pre><code class=\"prettyprint prettyprinted python\">\n",
        "</code></pre> -->\n",
        "\n",
        "```python\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "```"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Exploratory Data Analysis</font></center></h3>\n",
        "<img src=\"images/world_cloud_raw.png\">"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Data Preprocessing: Clean up special characters, punctuations, etc.</font></center></h3>\n",
        "\n",
        "```python\n",
        "#Convert to a list of all lowercase texts\n",
        "comments_orign = df_reviews['Text'].map(lambda x: x.lower()).values.tolist()\n",
        "\n",
        "#Replace null with None \n",
        "comments = [re.sub(r\"null|n/a.\", \"None\", i) for i in comments_orign]\n",
        "\n",
        "#Remove new line characters\n",
        "comments = [re.sub('\\s+', ' ', i) for i in comments]\n",
        "\n",
        "#Remove html line breaks\n",
        "comments = [re.sub(\"<br />|<br />|</a>|<a href=\", \" \", i) for i in comments]\n",
        "    \n",
        "#Remove punctuations\n",
        "comments = [re.sub('[,\\.;!?>]', ' ', i) for i in comments]\n",
        "    \n",
        "#Normalize spaces to 1\n",
        "comments = [re.sub(\" +\", \" \", i) for i in comments]\n",
        "```"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Data Preprocessing: Tokenization </font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>What's tokenization? Convert sentences to words</li>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "```python\n",
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sent), deacc=True)) #deacc = True removes punctuations \n",
        "\n",
        "cleaned_comments = list(sent_to_words(comments))\n",
        "```\n",
        "<pre><code class=\"prettyprint prettyprinted python\">\n",
        "\n",
        "#Example of cleaned comment:\n",
        "cleaned_comments[0]\n",
        "['bought','these','for','my','husband','who','is','currently','overseas','he','loves',\n",
        " 'these','and','apparently','his','staff','likes','them','also','there','are','generous',\n",
        " 'amounts','of','twizzlers','in','each','ounce','bag','and','this','was','well','worth',\n",
        " 'the','price','http','www','amazon','com','gp','product','gvisjm','twizzlers','strawberry',\n",
        " 'ounce','bags','pack','of']\n",
        "</code></pre>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Data Preprocessing: Remove stopwords</font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>What're stop words? Frequent words such as ‚Äùthe‚Äù, ‚Äùis‚Äù, etc. that do not have specific semantic</li>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.append(\"one\")\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[i for i in simple_preprocess(str(doc)) if i not in stop_words] for doc in texts]\n",
        "cleaned_comments_nonstops = remove_stopwords(cleaned_comments)\n",
        "```"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Data Preprocessing: Create Bigram</font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>What's bigram? Each token has two characters: 'whole foods', 'peanut butter'</li>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "```python\n",
        "#Form Bigram \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "cleaned_comments_bigrams = make_bigrams(cleaned_comments_nonstops)\n",
        "```"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Data Preprocessing: Lemmatization</font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>What's lemmatization? It helps reduce words like ‚Äòstudies‚Äô, ‚Äòstudying‚Äô to a common base form or root word ‚Äòstudy‚Äô</li>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "```python\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "def lemmatize(texts):\n",
        "    text_out = []\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for text in texts:\n",
        "        lemmatized_output = [lemmatizer.lemmatize(get_lemma(w)) for w in text]\n",
        "        text_out.append(lemmatized_output)\n",
        "    return text_out\n",
        "\n",
        "final_data = lemmatize(cleaned_comments_bigrams)\n",
        "```\n",
        "<!-- <pre><code class=\"prettyprint prettyprinted python\">\n",
        "</code></pre> -->"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Feature Extraction: Create Corpus, Dictionary and Term Document Frequency</font></center></h3>\n",
        "<div style=\"font-size: 20px\">\n",
        "<li>Corpus: equivalent to ‚Äúdataset‚Äù and is a collection of (data) texts</li>\n",
        "<li>Dictionary: a list of unique words in the text corpus</li>\n",
        "<li>Bag of Words: a dictionary from processed docs containing the number of times a word appears in the training dataset</li>\n",
        "</div>\n",
        "\n",
        "<hr>\n",
        "\n",
        "```python\n",
        "from gensim import corpora\n",
        "\n",
        "#Create Corpus\n",
        "texts = final_data\n",
        "\n",
        "#Create Dictionary \n",
        "id2word = corpora.Dictionary(final_data)\n",
        "\n",
        "#Bag of words \n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "```\n",
        "\n",
        "<!-- <pre><code class=\"prettyprint prettyprinted python\">\n",
        "#Example of bag of words\n",
        "[[(6, 2), (89, 1), (148, 1), (163, 1), (211, 1), (227, 1), (269, 1),\n",
        "  (455, 1), (531, 1), (754, 1), (1062, 1), (1112, 1), (1113, 1), (1114, 1),\n",
        "  (1115, 1), (1116, 1)],\n",
        " [(4, 2), (6, 1), (87, 1), (96, 1), (1082, 1), (1117, 1), (1118, 1)]]\n",
        "</code></pre> -->"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Feature Extraction: Create Corpus, Dictionary and Term Document Frequency</font></left></h3>\n",
        "\n",
        "![vector_space.png](attachment:vector_space.png)"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Parallelized computing of optimal number of topics</font></center></h3>\n",
        "\n",
        "```python\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "\n",
        "def compute_coherence_values(dictionary, corpus, texts, max_topics, start, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    \n",
        "    for num_topics in range(start, max_topics, step):\n",
        "        lda_model = <b>LdaMulticore</b>(corpus=corpus, num_topics=num_topics, id2word=id2word, \n",
        "                         workers=30, eval_every = 5,chunksize = 1000, \n",
        "                         passes =10, alpha = 'symmetric', per_word_topics = True)\n",
        "        print(num_topics)\n",
        "        model_list.append(lda_model)\n",
        "        coherencemodel = CoherenceModel(model = lda_model, \n",
        "                                        texts= texts, \n",
        "                                        dictionary = dictionary,\n",
        "                                        coherence = 'c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        \n",
        "    return model_list, coherence_values\n",
        " \n",
        "#Run parallelized computing to find otimal number of topics\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
        "                                                        corpus = corpus,\n",
        "                                                        texts = final_data,\n",
        "                                                        start=2,\n",
        "                                                        max_topics=20,\n",
        "                                                        step=1)\n",
        "```"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Visualize optimal number of topics</font></center></h3>\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "limit=11; start=2; step=1;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics -- LDA\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "<hr>\n",
        "<img src=\"cv_plot.png\">"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Print the optimal model (8 topics)</font></center></h3>\n",
        "<pre><code class=\"prettyprint prettyprinted python\">\n",
        "[(0,\n",
        "  '0.028*\"coffee\" + 0.014*\"great\" + 0.011*\"taste\" + 0.011*\"product\" + '\n",
        "  '0.009*\"good\" + 0.009*\"like\" + 0.009*\"love\" + 0.008*\"make\" + 0.008*\"cup\" + '\n",
        "  '0.007*\"flavor\"'),\n",
        " (1,\n",
        "  '0.020*\"taste\" + 0.015*\"like\" + 0.013*\"good\" + 0.011*\"try\" + 0.009*\"love\" + '\n",
        "  '0.009*\"product\" + 0.008*\"tea\" + 0.008*\"make\" + 0.007*\"get\" + '\n",
        "  '0.007*\"coffee\"'),\n",
        " (2,\n",
        "  '0.015*\"flavor\" + 0.014*\"like\" + 0.011*\"get\" + 0.011*\"chip\" + 0.009*\"food\" + '\n",
        "  '0.009*\"dog\" + 0.009*\"taste\" + 0.008*\"buy\" + 0.008*\"bag\" + 0.008*\"would\"'),\n",
        " (3,\n",
        "  '0.017*\"tea\" + 0.014*\"taste\" + 0.011*\"like\" + 0.010*\"flavor\" + '\n",
        "  '0.010*\"product\" + 0.008*\"love\" + 0.008*\"make\" + 0.007*\"get\" + 0.007*\"use\" + '\n",
        "  '0.007*\"food\"'),\n",
        " (4,\n",
        "  '0.019*\"food\" + 0.017*\"like\" + 0.014*\"dog\" + 0.010*\"good\" + 0.009*\"make\" + '\n",
        "  '0.008*\"product\" + 0.008*\"flavor\" + 0.007*\"amazon\" + 0.007*\"get\" + '\n",
        "  '0.007*\"love\"'),\n",
        " (5,\n",
        "  '0.017*\"like\" + 0.017*\"good\" + 0.010*\"product\" + 0.009*\"taste\" + '\n",
        "  '0.008*\"would\" + 0.008*\"make\" + 0.008*\"love\" + 0.008*\"price\" + 0.008*\"great\" '\n",
        "  '+ 0.007*\"bag\"'),\n",
        " (6,\n",
        "  '0.012*\"use\" + 0.012*\"product\" + 0.011*\"try\" + 0.009*\"get\" + 0.008*\"coffee\" '\n",
        "  '+ 0.007*\"taste\" + 0.007*\"flavor\" + 0.007*\"love\" + 0.007*\"like\" + '\n",
        "  '0.007*\"time\"'),\n",
        " (7,\n",
        "  '0.019*\"flavor\" + 0.015*\"great\" + 0.014*\"like\" + 0.012*\"love\" + 0.010*\"cup\" '\n",
        "  '+ 0.010*\"dog\" + 0.010*\"get\" + 0.010*\"coffee\" + 0.009*\"try\" + 0.008*\"use\"')]\n",
        "</code></pre>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Evaluation & Visualization: optimal model (8 topics)</font></center></h3>\n",
        "\n",
        "```python\n",
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "from pyLDAvis import gensim\n",
        "\n",
        "lda_display = gensim.prepare(optimal_model, corpus, id2word, sort_topics=False)\n",
        "pyLDAvis.display(lda_display)\n",
        "```\n",
        "\n",
        "<hr>\n",
        "<!-- <p><a href=\"http://cpsievert.github.io/LDAvis/reviews/vis/#topic=7&lambda=0.6&term=\">LDAVis</a></p>\n",
        "<img src=\"images/ldavis.png\"  width=\"80%\"> -->\n",
        "<div style=\"font-size: 18px\">\n",
        "<a href=\"http://30.161.120.109:45005/view/Disc2_QC/Data/amazon_reviews_lda8.html\">Visualizing 8 topics with pyLDAvis</a>\n",
        "\n",
        "<h4><left> <font color='green'>Resources</font></center></h4>\n",
        "<p><a href=\"https://github.com/cpsievert/LDAvis\">https://github.com/cpsievert/LDAvis</a>, \n",
        "   <a href=\"https://github.com/bmabey/pyLDAvis\">https://github.com/bmabey/pyLDAvis</a></p>\n",
        "</p></div>\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Print the model with 4 topics</font></center></h3>\n",
        "\n",
        "<pre><code class=\"prettyprint prettyprinted python\">\n",
        "[(0,\n",
        "  '0.021*\"like\" + 0.014*\"taste\" + 0.011*\"love\" + 0.010*\"good\" + 0.009*\"flavor\" '\n",
        "  '+ 0.009*\"coffee\" + 0.008*\"get\" + 0.008*\"food\" + 0.008*\"try\" + 0.007*\"make\"'),\n",
        " (1,\n",
        "  '0.012*\"flavor\" + 0.012*\"taste\" + 0.012*\"like\" + 0.011*\"product\" + '\n",
        "  '0.010*\"love\" + 0.009*\"buy\" + 0.008*\"good\" + 0.007*\"amazon\" + 0.007*\"try\" + '\n",
        "  '0.007*\"order\"'),\n",
        " (2,\n",
        "  '0.013*\"great\" + 0.012*\"product\" + 0.011*\"good\" + 0.011*\"make\" + '\n",
        "  '0.010*\"taste\" + 0.010*\"flavor\" + 0.008*\"like\" + 0.008*\"use\" + 0.007*\"dog\" + '\n",
        "  '0.007*\"love\"'),\n",
        " (3,\n",
        "  '0.011*\"like\" + 0.011*\"coffee\" + 0.010*\"try\" + 0.009*\"tea\" + 0.009*\"taste\" + '\n",
        "  '0.009*\"get\" + 0.008*\"flavor\" + 0.008*\"food\" + 0.008*\"product\" + '\n",
        "  '0.008*\"good\"')]\n",
        "</code></pre>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Evaluation & Visualization: the model with 4 topics</font></center></h3>\n",
        "\n",
        "<div style=\"font-size: 18px\">\n",
        "<a href=\"http://30.161.120.109:45005/view/Disc2_QC/Data/amazon_reviews_lda4.html\">Visualizing 4 topics using pyLDAvis</a>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Get dominant topic & prob and keywrods</font></center></h3>\n",
        "\n",
        "```python\n",
        "lda_topics = lda_model[corpus][0:len(corpus)]\n",
        "\n",
        "dominant_topic_lst=[]\n",
        "topic_prob_lst = []\n",
        "keywords_lst = []\n",
        "\n",
        "start = time.time()\n",
        "for i, (a1,b1,c1) in enumerate(lda_topics):\n",
        "    dominant_topic_lst.append(sorted(a1, key=itemgetter(1), reverse=True)[0][0])\n",
        "    topic_prob_lst.append(str(round(sorted(a1, key=itemgetter(1), reverse=True)[0][1],4)))\n",
        "    \n",
        "    wp = lda_model.show_topic(sorted(a1, key=itemgetter(1), reverse=True)[0][0])\n",
        "    topic_keywords = \", \".join([word for word, prop in wp])\n",
        "    keywords_lst.append(topic_keywords)\n",
        "    if i!=0 and i%1000==0:\n",
        "        print(i)\n",
        "        with open('domaint_topic_lst5.txt', 'w') as filehandle1:\n",
        "            json.dump(dominant_topic_lst, filehandle1)\n",
        "        with open('topic_prob_lst5.txt', 'w') as filehandle1:\n",
        "            json.dump(topic_prob_lst, filehandle1)\n",
        "        with open('keywords_lst5.txt', 'w') as filehandle1:\n",
        "            json.dump(keywords_lst, filehandle1)\n",
        "print('Total time to get dominant_topic: ' + str((time.time() - start)/60) + ' mins')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><left> <font color='green'>Get dominant topic, probability and keywrods</font></center></h3>\n",
        "\n",
        "![final_output3.png](attachment:final_output3.png)\n",
        "<hr>\n",
        "\n",
        "<h4><left> <font color='green'>Validation with an example</font></left></h4>\n",
        "<div style=\"font-size: 14px\">\n",
        "<p><b>Raw Review</b>: \"Since we've had a keurig, we've gone through many varieties of kcup <span class=\"fragment highlight-red\">\"coffee\"</span> including flavored and non flavored. Every kcup seemed to be either weak, bitter, acidic, or just ordinary. We have found OUR <span class=\"fragment highlight-red\">\"FAVORITE\"</span> with this one. It's strong, <span class=\"fragment highlight-red\">\"flavorful\"</span>, and smooth. Comes exactly as described by the seller, order promptly processed, arrives quickly. Can't beat it in our opinion. I can recommend this coffee and the seller with confidence.\"</p>\n",
        "<p><b>Keyword extracted from model</b>: flavor, great, like, love, cup, dog, get, coffee, try, use</p>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> <font color='green'>Other algorithms</font></center></h1>\n",
        "\n",
        "<div style=\"font-size: 18px\">\n",
        "<p><b>Latent Semantic Analysis (LSA)</b>: typically replace raw counts in the document-term matrix with a TF-IDF score</p>\n",
        "<p><b>Probabilistic Latent Semantic Analysis (pLSA)</b>: uses a probabilistic method instead of SVD to tackle the problem</p>\n",
        "<p><b>lda2vec</b>: an extension of word2vec and LDA that jointly learns word, document, and topic vectors</p>\n",
        "</div>"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> <font color='green'>Discussion</font></center></h1>\n",
        "\n",
        "![Q_A_Pic.jpg](attachment:Q_A_Pic.jpg)\n",
        "\n",
        "<hr>\n",
        "\n",
        "<div style=\"font-size: 20px\"><center>\n",
        "    Feedback, Questions? <b>Email: chenqianhugh@gmail.com </b>\n",
        "</center></div>\n"
      ],
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
